# DTS,PTS的概念
## 前提
在码率不变的前提下，GOP值越大，P、B帧的数量会越多，平均每个I、P、B帧所占用的字节数就越多，也就更容易获取较好的图像质量；Reference越大，B帧的数量越多，同理也更容易获得较好的图像质量。

需要说明的是，通过提高GOP值来提高图像质量是有限度的，在遇到场景切换的情况时，H.264编码器会自动强制插入一个I帧，此时实际的GOP值被缩短了。另一方面，在一个GOP中，P、B帧是由I帧预测得到的，当I帧的图像质量比较差时，会影响到一个GOP中后续P、B帧的图像质量，直到下一个GOP开始才有可能得以恢复，所以GOP值也不宜设置过大。

同时，由于P、B帧的复杂度大于I帧，所以过多的P、B帧会影响编码效率，使编码效率降低。另外，过长的GOP还会影响Seek操作的响应速度，由于P、B帧是由前面的I或P帧预测得到的，所以Seek操作需要直接定位，解码某一个P或B帧时，需要先解码得到本GOP内的I帧及之前的N个预测帧才可以，GOP值越长，需要解码的预测帧就越多，seek响应的时间也越长。

## DTS、PTS 的概念
- DTS（Decoding Time Stamp）：即解码时间戳，这个时间戳的意义在于告诉播放器该在什么时候解码这一帧的数据。
- PTS（Presentation Time Stamp）：即显示时间戳，这个时间戳用来告诉播放器该在什么时候显示这一帧的数据。
需要注意的是：虽然 DTS、PTS 是用于指导播放端的行为，但它们是在编码的时候由编码器生成的。

当视频流中没有 B 帧时，通常 DTS 和 PTS 的顺序是一致的。但如果有 B 帧时，就回到了我们前面说的问题：解码顺序和播放顺序不一致了。

比如一个视频中，帧的显示顺序是：I B B P，现在我们需要在解码 B 帧时知道 P 帧中信息，因此这几帧在视频流中的顺序可能是：I P B B，这时候就体现出每帧都有 DTS 和 PTS 的作用了。DTS 告诉我们该按什么顺序解码这几帧图像，PTS 告诉我们该按什么顺序显示这几帧图像。顺序大概如下：
```sh
PTS:   480   640  560  520  600  800  720  680  760  960 ...
DTS:   400   440  480  520  560  600  640  680  720  760 ...
Stream: I     P    B    B    B    P    B    B    B    P  ...
播放序： 1     5    3    2    4    9    7    6    8   10  ...               
PTS >= DTS
```

## 音视频的同步
上面说了视频帧、DTS、PTS 相关的概念。我们都知道在一个媒体流中，除了视频以外，通常还包括音频。音频的播放，也有 DTS、PTS 的概念，但是音频没有类似视频中 B 帧，不需要双向预测，所以音频帧的 DTS、PTS 顺序是一致的。

音频视频混合在一起播放，就呈现了我们常常看到的广义的视频。在音视频一起播放的时候，我们通常需要面临一个问题：怎么去同步它们，以免出现画不对声的情况。

要实现音视频同步，通常需要选择一个参考时钟，参考时钟上的时间是线性递增的，编码音视频流时依据参考时钟上的时间给每帧数据打上时间戳。在播放时，读取数据帧上的时间戳，同时参考当前参考时钟上的时间来安排播放。这里的说的时间戳就是我们前面说的 PTS。实践中，我们可以选择：同步视频到音频、同步音频到视频、同步音频和视频到外部时钟。

## PTS和DTS的时间基
PST和DTS的单位是什么?

为了回答这个问题，先引入FFmpeg中时间基的概念，也就是time_base。它也是用来度量时间的。如果把1秒分为25等份，你可以理解就是一把尺，那么每一格表示的就是1/25秒。此时的time_base={1，25},如果你是把1秒分成90000份，每一个刻度就是1/90000秒，此时的time_base={1，90000}。所谓时间基表示的就是每个刻度是多少秒

pts的值就是占多少个时间刻度（占多少个格子）。它的单位不是秒，而是时间刻度。只有pts加上time_base两者同时在一起，才能表达出时间是多少。

好比我只告诉你，某物体的长度占某一把尺上的20个刻度。但是我不告诉你，这把尺总共是多少厘米的，你就没办法计算每个刻度是多少厘米，你也就无法知道物体的长度。

pts=20个刻度

time_base={1,10},每一个刻度是1/10厘米

所以物体的长度=pts\*time_base=20\*1/10 厘米

在ffmpeg中:av_q2d(time_base)=每个刻度是多少秒

此时你应该不难理解 pts*av_q2d(time_base)才是帧的显示时间戳。

下面理解时间基的转换，为什么要有时间基转换。 
首先，不同的封装格式，timebase是不一样的。另外，整个转码过程，不同的数据状态对应的时间基也不一致。拿mpegts封装格式25fps来说（只说视频，音频大致一样，但也略有不同）。非压缩时候的数据（即YUV或者其它），在ffmpeg中对应的结构体为AVFrame,它的时间基为AVCodecContext 的time_base ,AVRational{1,25}。

压缩后的数据（对应的结构体为AVPacket）对应的时间基为AVStream的time_base，AVRational{1,90000}。

因为数据状态不同，时间基不一样，所以我们必须转换，在1/25时间刻度下占10格，在1/90000下是占多少格。这就是pts的转换。

根据pts来计算一桢在整个视频中的时间位置：

timestamp(秒) = pts \* av_q2d(st->time_base)

duration和pts单位一样，duration表示当前帧的持续时间占多少格。或者理解是两帧的间隔时间是占多少格。一定要理解单位。

pts：格子数

av_q2d(st->time_base): 秒/格

计算视频长度：

time(秒) = st->duration \* av_q2d(st->time_base)

ffmpeg内部的时间与标准的时间转换方法：

ffmpeg内部的时间戳 = AV_TIME_BASE \* time(秒)

AV_TIME_BASE_Q=1/AV_TIME_BASE

av_rescale_q(int64_t a, AVRational bq, AVRational cq)函数

这个函数的作用是计算a\*bq / cq来把时间戳从一个时间基调整到另外一个时间基。在进行时间基转换的时候，应该首先这个函数，因为它可以避免溢出的情况发生。

函数表示在bq下的占a个格子，在cq下是多少。

关于音频pts的计算：

音频sample_rate:samples per second，即采样率，表示每秒采集多少采样点。

比如44100HZ，就是一秒采集44100个sample.即每个sample的时间是1/44100秒

一个音频帧的AVFrame有nb_samples个sample，所以一个AVFrame耗时是nb_samples \*（1/44100）秒,即标准时间下duration_s=nb_samples \*（1/44100）秒,转换成AVStream时间基下duration=duration_s / av_q2d(st->time_base);

基于st->time_base的num值一般等于采样率,所以duration=nb_samples. 

pts=n\*duration=n\*nb_samples